ğŸ•·ï¸ Proyecto de Web Scraping con CI/CD
Este proyecto estÃ¡ diseÃ±ado para realizar scraping de datos desde fuentes especÃ­ficas y automatizar el proceso utilizando GitHub Actions para la integraciÃ³n continua. Desarrollado en Python, permite la ejecuciÃ³n programada y la validaciÃ³n del cÃ³digo en cada cambio.

ğŸš€ CaracterÃ­sticas
Scraping automatizado: ExtracciÃ³n de datos desde pÃ¡ginas web seleccionadas.

IntegraciÃ³n continua: ValidaciÃ³n de cÃ³digo y ejecuciÃ³n de pruebas con cada cambio.

EjecuciÃ³n programada: AutomatizaciÃ³n del scraping en intervalos definidos.

ExportaciÃ³n de datos: Almacenamiento de los datos extraÃ­dos en formatos como JSON o CSV.

ğŸ› ï¸ TecnologÃ­as utilizadas
Python 3.x

LibrerÃ­as: requests, BeautifulSoup, Selenium, Scrapy (ajustar segÃºn tu implementaciÃ³n)

GitHub Actions: Para la automatizaciÃ³n de flujos de trabajo.

ğŸ“ Estructura del proyecto
.
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ docker.yml
â”œâ”€â”€ docs/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ edu_pad/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ dockerfile
â”œâ”€â”€ integracionyml_old.txt
â”œâ”€â”€ mainpy_old.txt
â”œâ”€â”€ mainyml_old.txt
â”œâ”€â”€ pruebadocker.py
â”œâ”€â”€ setup.py
â””â”€â”€ requirements.txt

ğŸ”„ Flujo de trabajo CI (GitHub Actions)
Cada vez que se realiza un push o pull request a la rama main, se ejecuta un flujo de trabajo automatizado que:

Instala las dependencias necesarias.

Ejecuta linters para asegurar la calidad del cÃ³digo.

Ejecuta pruebas automatizadas.

Realiza el scraping y almacena los datos extraÃ­dos.

â–¶ï¸ CÃ³mo ejecutar localmente
git clone https://github.com/1J3G/pad_2025_1_2JG.git
cd pad_2025_1_2JG
pip install -r requirements.txt
python src/edu_pad/main.py

ğŸ“„ Licencia
Este proyecto estÃ¡ licenciado bajo la Licencia MIT. Consulta el archivo LICENSE para mÃ¡s detalles.
